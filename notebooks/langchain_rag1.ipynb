{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain Retrieval Augmented Generation Example 1\n",
        "\n",
        "[Build a Retrieval Augmented Generation (RAG) App: Part 1](https://python.langchain.com/docs/tutorials/rag/)\n",
        "\n",
        "Introduce the basic components of a RAG system and how they should be connected, including \n",
        "* document loader, \n",
        "* document splitter, \n",
        "* embedding model, \n",
        "* LLM model, \n",
        "* vector store,\n",
        "* LangChain workflow\n",
        "    * query analysis step\n",
        "    * document retrieval\n",
        "    * generator"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import dotenv\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "## Setup Environment\n",
        "sys.path.append(Path.cwd().parent) # Append project home to system path\n",
        "dotenv.load_dotenv() # Load .env"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1740341523619
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construct LLM"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.vectorstores import InMemoryVectorStore"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1740341526410
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM\n",
        "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
        "\n",
        "# Embeding\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1740341526670
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Documents"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import List, TypedDict"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1740341527389
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and chunk contents of the blog\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "docs = loader.load()\n",
        "print(docs[0].page_content[:500])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n\n      LLM Powered Autonomous Agents\n    \nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n\n\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1740341527746
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Documents \n",
        "\n",
        "The RecursiveCharacterTextSplitter is a text splitting utility provided by LangChain designed to break down long documents into smaller, manageable chunks while trying to preserve semantic coherence. It recursively splits the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n",
        "\n",
        "The algorithm begins by trying to split the text using the largest or most meaningful separator (for example, a double newline or paragraph break). If a chunk is still too long, it recursively applies smaller separators (such as single newline characters, punctuation, or even individual characters) until the chunks meet the desired length.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split document using RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "all_splits[:5]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'),\n Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.'),\n Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'),\n Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.')]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1740341528047
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Store Documents\n",
        "\n",
        "Document chunk are stored in the vector database. \n",
        "\n",
        "Each document chunk consists of content and metadata. By default, the metadata contains the `source` for the original document of the chunk. We can also add other metadata to document chunk. For example, here we add the position of the chunk in the document."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Attach metadata to the document\n",
        "total_documents = len(all_splits)\n",
        "third = total_documents // 3\n",
        "\n",
        "for i, document in enumerate(all_splits):\n",
        "    if i < third:\n",
        "        document.metadata[\"section\"] = \"beginning\"\n",
        "    elif i < 2 * third:\n",
        "        document.metadata[\"section\"] = \"middle\"\n",
        "    else:\n",
        "        document.metadata[\"section\"] = \"end\"\n",
        "\n",
        "\n",
        "all_splits[2].metadata"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 33,
          "data": {
            "text/plain": "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n 'section': 'beginning'}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 33,
      "metadata": {
        "gather": {
          "logged": 1740343291919
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add document chunks into the vector store. \n",
        "\n",
        "Here we used an in-memory vector datastore for demonstration purposes."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vector store\n",
        "vector_store = InMemoryVectorStore(embeddings)"
      ],
      "outputs": [],
      "execution_count": 48,
      "metadata": {
        "gather": {
          "logged": 1740343599803
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Index chunks\n",
        "document_ids = vector_store.add_documents(documents=all_splits)\n",
        "\n",
        "print(\"Total documents: %d\" % (len(vector_store.store)))\n",
        "print(\"Top three document IDs: %s\" % (str(document_ids[:3])))\n",
        "print(\"Example document: %s\" % (str(vector_store.get_by_ids([document_ids[1]]))))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Total documents: 66\nTop three document IDs: ['78382b8e-4320-47a1-95bc-70e3c215868d', '5fc67bcb-4fab-4cc1-91bc-30a4a31cdcd0', '28e12b5d-cf79-42e1-beb9-894f7495b923']\nExample document: [Document(id='5fc67bcb-4fab-4cc1-91bc-30a4a31cdcd0', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'beginning'}, page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.')]\n"
        }
      ],
      "execution_count": 49,
      "metadata": {
        "gather": {
          "logged": 1740343601299
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG Chain\n",
        "\n",
        "Retrieve and Generation"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query Analysis Step\n",
        "\n",
        "Query analyzing is a useful step. It ananlyzes the user input question using LLM and generates a structured RAG queries to help retrieval.  \n",
        "For example, some user input questions are multifaceted, including irrelavent context, or with filters. These questions need to be analyzed and rewritten for the RAG system to more accurate retrieval. For input question with filters, such as \"Find document since year 2020\", the year element is extracted by the query analyzer, and is evaluated as filter to only select documents published after 2020 to query. \n",
        "\n",
        "A few key points to implementation of a query analysis step.\n",
        "* Define the a structured `Search` object using `TypeDict` and `Annotated` typed fields.\n",
        "* The use of `Annotated` is purely for documentation or external tools—it doesn't change runtime behavior but helps guide LLM outputs. \n",
        "* The annotation is the query which LLM uses to extract and populate the fields.\n",
        "* THe function `llm.with_structured_output(Search)` modifies the behavior of the LLM so that it generates output conforming to the Search schema. Instead of returning a free-text response, the LLM formats the response as a dictionary matching `Search`.\n",
        "\n",
        "Note: Multifacted query example - can you help me find information on the latest trends in artificial intelligence, including recent research papers, key industry players, and potential ethical concerns?\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "from typing_extensions import Annotated"
      ],
      "outputs": [],
      "execution_count": 50,
      "metadata": {
        "gather": {
          "logged": 1740343610835
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define search query\n",
        "class Search(TypedDict):\n",
        "    \"\"\"Search query. Field annotation are used by the LLM to populate value.\"\"\"\n",
        "    query: Annotated[str, ..., \"Search query to run.\"]\n",
        "    section: Annotated[\n",
        "        Literal[\"beginning\", \"middle\", \"end\"],\n",
        "        ...,\n",
        "        \"Section to query.\",\n",
        "    ]\n",
        "\n",
        "# Define state for application\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    query: Search\n",
        "    context: List[Document]\n",
        "    answer: str"
      ],
      "outputs": [],
      "execution_count": 51,
      "metadata": {
        "gather": {
          "logged": 1740343612480
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define analyze query step\n",
        "def analyze_query(state: State):\n",
        "    \"\"\"Use LLM to analyze the user question and populate a `Search` object based on the annotation of each field.\"\"\"\n",
        "    structured_llm = llm.with_structured_output(Search)\n",
        "    query = structured_llm.invoke(state[\"question\"])\n",
        "    return {\"query\": query}"
      ],
      "outputs": [],
      "execution_count": 76,
      "metadata": {
        "gather": {
          "logged": 1740345817962
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieval Step\n",
        "\n",
        "Use LangGraph to create the Retrieval-Generation chain. LangGraph provides an integrated method to build business logic into a workflow. It also allows human-in-the-loop. To use LangGraph, we need to define three things:\n",
        "* The State of our application;\n",
        "* The steps of our application;\n",
        "* The workflow of our application.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END"
      ],
      "outputs": [],
      "execution_count": 53,
      "metadata": {
        "gather": {
          "logged": 1740343615007
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define application steps\n",
        "def retrieve(vector_store: InMemoryVectorStore):\n",
        "    \"\"\"The retrieve function takes the current application state and performs a similarity \n",
        "    search on a vector store using the question provided in the state. It collects relevant \n",
        "    documents based on semantic similarity. The function then returns these documents as \n",
        "    context for further processing.\"\"\"\n",
        "    def _retrieve(state: State):\n",
        "        query = state['query']\n",
        "        retrieved_docs = vector_store.similarity_search(\n",
        "            query[\"query\"],\n",
        "            filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],\n",
        "        )\n",
        "        return {\"context\": retrieved_docs}\n",
        "    return _retrieve"
      ],
      "outputs": [],
      "execution_count": 71,
      "metadata": {
        "gather": {
          "logged": 1740344135135
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation Step\n",
        "\n",
        "At the generation step, the RAG system stitch the retrieved context information with rewritten questions for generation.\n",
        "\n",
        "Pre-defined prompts can be pulled from Langchain Hub. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a simple generation prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# Print out the prompt\n",
        "example_messages = prompt.invoke(\n",
        "    {\n",
        "        \"context\": \"(context goes here)\",\n",
        "        \"question\": \"(question goes here)\"\n",
        "    }\n",
        ").to_messages()\n",
        "print(example_messages[0].content)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: (question goes here) \nContext: (context goes here) \nAnswer:\n"
        }
      ],
      "execution_count": 68,
      "metadata": {
        "gather": {
          "logged": 1740344040465
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively generation prompt can also be defined by users."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternatively the prompt template can be loaded from user-defined\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "Always say \"thanks for asking!\" at the end of the answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)"
      ],
      "outputs": [],
      "execution_count": 57,
      "metadata": {
        "gather": {
          "logged": 1740343640361
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the generation step."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(state: State):\n",
        "    \"\"\"The generation function concatenates the content of the retrieved documents from the \n",
        "    state into a single text block. It then creates a prompt using the question and the \n",
        "    compiled context, and passes this prompt to a language model for response generation. \n",
        "    Finally, it extracts and returns the generated answer.\n",
        "    \"\"\"\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"answer\": response.content}"
      ],
      "outputs": [],
      "execution_count": 58,
      "metadata": {
        "gather": {
          "logged": 1740343641923
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build RAG Chain and Invoke\n",
        "\n",
        "This code chunk builds a workflow graph for the application. It creates a state-based graph using the State type and adds the retrieve and generate functions in sequence, establishing the order of operations. An edge is then added from a designated start node to the retrieve step, setting the entry point of the workflow. Finally, the graph is compiled into an executable structure that orchestrates the application's logic."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# Compile a simple RAG system\n",
        "graph_builder = StateGraph(State) # Start a graph builder\n",
        "graph_builder.add_sequence([\n",
        "    analyze_query, \n",
        "    retrieve(vector_store), \n",
        "    generate\n",
        "]) # Add nodes\n",
        "graph_builder.add_edge(START, \"analyze_query\") # Build graph\n",
        "graph_builder.add_edge(\"generate\", END) # Build graph\n",
        "graph = graph_builder.compile() # Compile\n",
        "\n",
        "# Display the application\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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",
            "text/plain": "<IPython.core.display.Image object>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 72,
      "metadata": {
        "gather": {
          "logged": 1740344140507
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke a LangGraph\n",
        "# The invoke mode outputs all steps and output all outputs at once after the last step is executed.\n",
        "response = graph.invoke({\"question\": \"What does the end of the post say about Task Decomposition?\"})\n",
        "\n",
        "print(response[\"context\"], \"\\n\\n\")\n",
        "print(response[\"answer\"])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[Document(id='33414b9f-65b3-4635-a84b-476c4d19a2fe', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'end'}, page_content='You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.'), Document(id='61b9f192-a5a5-4f67-94f2-75d0c9bdbc5c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'end'}, page_content='\"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully'), Document(id='bfb8ee53-37fe-4eea-8c2b-e7e1f68f0cc8', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'end'}, page_content='\"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease'), Document(id='c2cec7ff-a188-48e8-aee0-7c0686fb2d65', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'end'}, page_content='FILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.')] \n\n\nThe end of the post emphasizes the importance of detailing every aspect of the architecture through code implementation. It instructs to take a step-by-step approach, ensuring all core classes, functions, and methods are identified and documented. Additionally, it stresses the need for completeness and compatibility across different files, adhering to best practices in file naming and structure.\n"
        }
      ],
      "execution_count": 73,
      "metadata": {
        "gather": {
          "logged": 1740344146246
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stream updates a LangGraph\n",
        "# The stream mode execute the application steps one by one and output after each step.\n",
        "for step in graph.stream(\n",
        "    {\"question\": \"What does the end of the post say about Task Decomposition?\"}, \n",
        "    stream_mode=\"updates\"\n",
        "):\n",
        "    print(f\"{step}\\n\\n-----------------------------\\n\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'analyze_query': {'query': {'query': 'Task Decomposition', 'section': 'end'}}}\n\n-----------------------------\n\n{'_retrieve': {'context': [Document(id='33414b9f-65b3-4635-a84b-476c4d19a2fe', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'end'}, page_content='You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.'), Document(id='61b9f192-a5a5-4f67-94f2-75d0c9bdbc5c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'end'}, page_content='\"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully'), Document(id='bfb8ee53-37fe-4eea-8c2b-e7e1f68f0cc8', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'end'}, page_content='\"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease'), Document(id='c2cec7ff-a188-48e8-aee0-7c0686fb2d65', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'end'}, page_content='FILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.')]}}\n\n-----------------------------\n\n{'generate': {'answer': 'The end of the post emphasizes the importance of task decomposition by insisting that every detail of the architectural design must be appropriately broken down and implemented into functional code. It outlines a systematic approach that starts with identifying core components and progressively detailing their implementation in code. The instructions encourage thoroughness to ensure all aspects of the architecture are addressed and compatible across different files.'}}\n\n-----------------------------\n\n"
        }
      ],
      "execution_count": 74,
      "metadata": {
        "gather": {
          "logged": 1740344223091
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stream tokens\n",
        "for message, metadata in graph.stream(\n",
        "    {\"question\": \"What does the end of the post say about Task Decomposition?\"}, \n",
        "    stream_mode=\"messages\"\n",
        "):\n",
        "    print(message.content, end=\"|\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "|{\"|query|\":\"|Task| De|composition|\",\"|section|\":\"|end|\"}|||||The| end| of| the| post| emphasizes| that| Task| De|composition| involves| thoroughly| planning| and| detailing| the| architecture| before| implementation|.| It| suggests| method|ically| identifying| core| classes|,| functions|,| and| files|,| and| ensuring| that| every| aspect| is| fully| coded| without| placeholders|.| Additionally|,| it| highlights| the| importance| of| following| best| practices| for| file| naming| and| organization|,| and| ensuring| all| parts| of| the| architecture| are| compatible| and| well|-d|ocumented|.||"
        }
      ],
      "execution_count": 75,
      "metadata": {
        "gather": {
          "logged": 1740344272261
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "[LangGraph](https://langchain-ai.github.io/langgraph/)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "rag"
    },
    "kernelspec": {
      "name": "rag",
      "language": "python",
      "display_name": "Python (rag)"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}