{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GraphRAG\n",
        "\n",
        "The LLMs are good at sensemaking - to derive insights from a global knowledge. RAG is ideal when the total number of records in a data source is too large to include in a single prompt to the LLM. On the other side, RAG grounds the LLM generation to document chunks and can cause poor answers to questions requiring a holistic understand of the entire dataset. For example, questions like ”What are the key trends in how scientific discoveries are influenced by interdisciplinary research over the past decade?” often require the LLM to add the entire document into the context for generation.\n",
        "\n",
        "The article [Building a Graph RAG System: A Step-by-Step Approach](https://machinelearningmastery.com/building-graph-rag-system-step-by-step-approach/) also mentioned that documents retrieved by regular RAG systems can lack of dependencies and cause the answers generated by the LLM to be fragmented. This article used a good example for the fragmented answer generated by regular RAG.\n",
        "\n",
        "    In a traditional RAG setup, the system might retrieve the following pieces of information:\n",
        "\n",
        "    Document 1: “James Watson and Francis Crick proposed the double-helix structure in 1953.”\n",
        "    Document 2: “Rosalind Franklin’s X-ray diffraction images were critical in identifying DNA’s helical structure.”\n",
        "    Document 3: “Maurice Wilkins shared Franklin’s images with Watson and Crick, which contributed to their discovery.”\n",
        "    The problem? Traditional RAG systems treat these documents as independent units. They don’t connect the dots effectively, leading to fragmented responses like: \n",
        "\n",
        "    “Watson and Crick proposed the structure, and Franklin’s work was important.”\n",
        "\n",
        "    This response lacks depth and misses key relationships between contributors. Enter Graph RAG! By organizing the retrieved data as a graph, Graph RAG represents each document or fact as a node, and the relationships between them as edges.\n",
        "\n",
        "    Here’s how Graph RAG would handle the same query:\n",
        "\n",
        "    Nodes: Represent facts (e.g., “Watson and Crick proposed the structure,” “Franklin contributed critical X-ray images”).\n",
        "    Edges: Represent relationships (e.g., “Franklin’s images → shared by Wilkins → influenced Watson and Crick”).\n",
        "    By reasoning across these interconnected nodes, Graph RAG can produce a complete and insightful response like:\n",
        "\n",
        "    “The discovery of DNA’s double-helix structure in 1953 was primarily led by James Watson and Francis Crick. However, this breakthrough heavily relied on Rosalind Franklin’s X-ray diffraction images, which were shared with them by Maurice Wilkins.”\n",
        "\n",
        "    This ability to combine information from multiple sources and answer broader, more complex questions is what makes Graph RAG so popular.\n",
        "\n",
        "\n",
        "[Building a Graph RAG System: A Step-by-Step Approach](https://machinelearningmastery.com/building-graph-rag-system-step-by-step-approach/)\n",
        "\n",
        "[From Local to Global: A GraphRAG Approach to Query-Focused Summarization](https://arxiv.org/pdf/2404.16130)\n",
        "\n",
        "[Using a Knowledge Graph to Implement a RAG application](https://www.datacamp.com/tutorial/knowledge-graph-rag)\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install \\\n",
        "# --upgrade \\\n",
        "# langchain \\\n",
        "# llama-index \\\n",
        "# langchain-community \\\n",
        "# langchain-openai \\\n",
        "# langchain-neo4j \\\n",
        "# neo4j"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1742069771971
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "# Get logger\n",
        "logger = logging.getLogger(\"rag\")\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Stream handler\n",
        "stream_handler = logging.StreamHandler()\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "stream_handler.setFormatter(formatter)\n",
        "logger.addHandler(stream_handler)\n",
        "\n",
        "logger.propagate = False\n"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1742069772168
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"Test\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2025-03-15 20:16:11,074 - rag - INFO - Test\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1742069772314
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG Building Blocks\n",
        "\n",
        "__Deploy Azure OpenAI Services__. This include an LLM deployment and an embedding deployment. In this case we deployed a gpt-4o-mini on the [AI Foundary workspace](https://oai.azure.com/resource/overview?wsid=/subscriptions/d91792a2-c9bd-44bc-bcd8-fdddc7ceb1c5/resourceGroups/agentic_applications/providers/Microsoft.CognitiveServices/accounts/multi-agentic-applications&tid=565f1c8e-754e-473e-8352-ac5b86a38c93). Set access key as environment variables for Langchain to access Azure services, including AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT and OPENAI_API_VERSION in .env file."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import dotenv\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "## Setup Environment\n",
        "sys.path.append(Path.cwd().parent) # Append project home to system path\n",
        "dotenv.load_dotenv(override=True) # Load .env"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1742069772520
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Test Azure Connection\n",
        "import openai\n",
        "\n",
        "client = openai.AzureOpenAI(\n",
        "    api_version=\"2025-01-01-preview\",\n",
        ")\n",
        "\n",
        "# gpt-4o-mini only support chat completion. Use client.chat.completions.create instead of\n",
        "# client.completions.create\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Test prompt\"}],\n",
        ")\n",
        "\n",
        "response"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "ChatCompletion(id='chatcmpl-BBSDVYFCGX0hnl1s3dpoRgf44xU1M', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"It looks like you're testing the functionality! How can I assist you today? If you have any questions or need help with something specific, feel free to let me know!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1742069773, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_b705f0c291', usage=CompletionUsage(completion_tokens=34, prompt_tokens=9, total_tokens=43, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1742069773544
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initiate RAG Building Blocks. In this case we used Langchain to initiate LLM, Embedding and Vector Store."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "# Connect to chat model. \n",
        "# Here we use AzureChatOpenAI instead AzureOpenAI to connect to gpt-4o-mini\n",
        "llm = AzureChatOpenAI(azure_deployment=\"gpt-4o-mini\")\n",
        "\n",
        "# Connect to embedding\n",
        "embeddings = AzureOpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "\n",
        "# Instantiate vector store\n",
        "vector_store = InMemoryVectorStore(embeddings)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1742069774730
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test azure connection\n",
        "llm.invoke(\"Tell me a joke\")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "AIMessage(content='Why did the scarecrow win an award?\\n\\nBecause he was outstanding in his field!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 11, 'total_tokens': 28, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b705f0c291', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-7f8d8cad-ff53-4a9c-9309-9c3dbcd13625-0', usage_metadata={'input_tokens': 11, 'output_tokens': 17, 'total_tokens': 28, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1742069775509
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunk Texts\n",
        "\n",
        "Chunk text into strings."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data\n",
        "import pandas as pd\n",
        "news = pd.read_csv(\"https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/news_articles.csv\")[:50]\n",
        "news[:1]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "                    title                                 date  \\\n0  Chevron: Best Of Breed  2031-04-06T01:36:32.000000000+00:00   \n\n                                                text  \n0  JHVEPhoto Like many companies in the O&G secto...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>date</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Chevron: Best Of Breed</td>\n      <td>2031-04-06T01:36:32.000000000+00:00</td>\n      <td>JHVEPhoto Like many companies in the O&amp;G secto...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1742069778571
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Convert string to Langchain document\n",
        "news_documents = [Document(row[1]['text']) for row in news.iterrows()]\n",
        "\n",
        "logger.info(\"Total documents: %d\", len(news_documents))\n",
        "news_documents[:1]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2025-03-15 20:16:18,568 - rag - INFO - Total documents: 50\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "[Document(metadata={}, page_content='JHVEPhoto Like many companies in the O&G sector, the stock of Chevron (NYSE:CVX) has declined about 10% over the past 90-days despite the fact that Q2 consensus earnings estimates have risen sharply (~25%) during that same time frame. Over the years, Chevron has kept a very strong balance sheet. That allowed the...')]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1742069778760
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splits text into chunks of 500 characters with a 100-character overlap to maintain context between chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "all_splits = text_splitter.split_documents(news_documents)\n",
        "\n",
        "logger.info(\"Total splits: %d\", len(all_splits))\n",
        "all_splits[:1]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2025-03-15 20:16:18,581 - rag - INFO - Total splits: 291\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "[Document(metadata={}, page_content='JHVEPhoto Like many companies in the O&G sector, the stock of Chevron (NYSE:CVX) has declined about 10% over the past 90-days despite the fact that Q2 consensus earnings estimates have risen sharply (~25%) during that same time frame. Over the years, Chevron has kept a very strong balance sheet. That allowed the...')]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1742069779004
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Knowledge Graph\n",
        "\n",
        "We can use a out-of-box graph transformer implemented in Langchain (langchain_experimental.graph_transformers.LLMGraphTransformer). Internally the graph transformer prompt the LLM to identify graph nodes and edges. \n",
        "\n",
        "The LLMGraphTransformer didn't finish in 25 minutes on 291 documents."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
        "\n",
        "# # Extract Knowledge Graph\n",
        "# llm_transformer = LLMGraphTransformer(llm=llm)\n",
        "# graph_documents = llm_transformer.convert_to_graph_documents(all_splits)"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1742051771702
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Neo4J deployment](https://portal.azure.com/#@weidavidtaigmail.onmicrosoft.com/resource/subscriptions/d91792a2-c9bd-44bc-bcd8-fdddc7ceb1c5/resourceGroups/agentic_ai/providers/Microsoft.Compute/virtualMachines/agenticai-neo4j-dev-eus-01/overview)\n",
        "\n",
        "[Neo4J deployment script](https://learn.microsoft.com/en-us/samples/azure/azure-quickstart-templates/neo4j-ubuntu-vm/?wt.mc_id=searchAPI_azureportal_inproduct_rmskilling&sessionId=69aea466ebeb4ed480787b95420f7983)\n",
        "\n",
        "[Neo4J Private Network Security Group](https://portal.azure.com/#@weidavidtaigmail.onmicrosoft.com/resource/subscriptions/d91792a2-c9bd-44bc-bcd8-fdddc7ceb1c5/resourceGroups/agentic_ai/providers/Microsoft.Network/networkSecurityGroups/agenticai-neo4j-dev-eus-01-nsg/overview) has network firewall rules.\n",
        "\n",
        "Command to ssh to Neo4J server for management from my Microsoft Laptop.\n",
        "\n",
        "```\n",
        "ssh -i ~/.ssh/agenticai-neo4j-dev-eus-01.pem weitai@20.185.223.219\n",
        "ssh -i ~/.ssh/agenticai-neo4j-dev-eus-01.pem weitai@agenticai-neo4j-dev-eus-01.eastus.cloudapp.azure.com\n",
        "```\n",
        "\n",
        "Neo4j management command\n",
        "\n",
        "```\n",
        "sudo systemctl enable neo4j # Set neo4j as a system service. Ensure that Neo4j automatically starts on boot\n",
        "sudo systemctl status neo4j # Check status of Neo4j\n",
        "sudo systemctl restart neo4j # Restart Neo4j\n",
        "sudo systemctl start neo4j # Start Neo4j\n",
        "```\n",
        "\n",
        "To enable Neo4J access from public IP, SSH to the Neo4J VM and edit Neo4J configuration file\n",
        "```\n",
        "sudo nano /etc/neo4j/neo4j.conf\n",
        "```\n",
        "Make sure these settings are enabled\n",
        "```\n",
        "dbms.default_listen_address=0.0.0.0 # Neo4j might only be listening on localhost (127.0.0.1) instead of 0.0.0.0 (all interfaces).\n",
        "\n",
        "dbms.connector.bolt.listen_address=:7687 # Bolt connection\n",
        "dbms.connector.http.listen_address=:7474 # HTTP connection\n",
        "dbms.connector.https.listen_address=:7473 # HTTPs connection\n",
        "```\n",
        "\n",
        "Neo4J can also be accessed through HTTP at http://agenticai-neo4j-dev-eus-01.eastus.cloudapp.azure.com:7474/browser/\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# graph_store.write_graph(graph_documents)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a Graph Transformer."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import Callable, Any, Optional, Union\n",
        "\n",
        "entity_pattern = r'entity_name:\\s*(.+?)\\s*entity_type:\\s*(.+?)\\s*entity_description:\\s*(.+?)\\s*'\n",
        "relationship_pattern = r'source_entity:\\s*(.+?)\\s*target_entity:\\s*(.+?)\\s*relation:\\s*(.+?)\\s*relationship_description:\\s*(.+?)\\s*'\n",
        " \n",
        "def parse_fn(response_str: str) -> Any:\n",
        "    entities = re.findall(entity_pattern, response_str)\n",
        "    relationships = re.findall(relationship_pattern, response_str)\n",
        "    return entities, relationships"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1742070361081
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 53,
          "data": {
            "text/plain": "[]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 53,
      "metadata": {
        "gather": {
          "logged": 1742076446064
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from llama_index.core.async_utils import run_jobs\n",
        "from llama_index.core.graph_stores.types import (\n",
        "    EntityNode,\n",
        "    KG_NODES_KEY,\n",
        "    KG_RELATIONS_KEY,\n",
        "    Relation,\n",
        ")\n",
        "from llama_index.core.indices.property_graph.utils import (\n",
        "    default_parse_triplets_fn,\n",
        ")\n",
        "from llama_index.core.prompts.default_prompts import (\n",
        "    DEFAULT_KG_TRIPLET_EXTRACT_PROMPT,\n",
        ")\n",
        "\n",
        "from langchain_core.language_models.chat_models import BaseChatModel\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "from langchain_community.graphs.networkx_graph import NetworkxEntityGraph\n",
        "\n",
        "from langchain.schema import Document, BaseDocumentTransformer\n",
        "\n",
        "class GraphRAGExtractor(BaseDocumentTransformer):\n",
        "   '''Extract triples from a graph.\n",
        "\n",
        "   Uses an LLM and a simple prompt + output parsing to extract paths (i.e. triples) and entity, relation descriptions from text.\n",
        "\n",
        "   Args:\n",
        "      llm (LLM):\n",
        "         The language model to use.\n",
        "      extract_prompt (Union[str, PromptTemplate]):\n",
        "         The prompt to use for extracting triples.\n",
        "      parse_fn (callable):\n",
        "         A function to parse the output of the language model.\n",
        "      num_workers (int):\n",
        "         The number of workers to use for parallel processing.\n",
        "      max_paths_per_chunk (int):\n",
        "         The maximum number of paths to extract per chunk.\n",
        "\n",
        "   '''\n",
        "   # Type hints\n",
        "   llm: BaseChatModel\n",
        "   extract_prompt: PromptTemplate\n",
        "   parse_fn: Callable\n",
        "   num_workers: int\n",
        "   max_knowledge_triples: int\n",
        "\n",
        "   # Class method\n",
        "   @classmethod\n",
        "   def class_name(cls) -> str:\n",
        "      return \"GraphRAGExtractor\"\n",
        "\n",
        "   def __init__(\n",
        "      self,\n",
        "      llm: Optional[BaseChatModel] = None,\n",
        "      extract_prompt: Optional[Union[str, PromptTemplate]] = None,\n",
        "      parse_fn: Callable = None,\n",
        "      num_workers: int = 4,\n",
        "      max_knowledge_triples: int = 10\n",
        "   ):\n",
        "      \"\"\"\n",
        "      Init params.\n",
        "      \"\"\"\n",
        "      super().__init__()\n",
        "      self.llm = llm\n",
        "      self.extract_prompt = extract_prompt or DEFAULT_KG_TRIPLET_EXTRACT_PROMPT\n",
        "      self.parse_fn = parse_fn or default_parse_triplets_fn\n",
        "      self.num_workers = num_workers\n",
        "      self.max_knowledge_triples = max_knowledge_triples\n",
        "\n",
        "   def transform_documents(\n",
        "      self, \n",
        "      documents: list[Document]\n",
        "   ) -> list[Document]:\n",
        "      \"\"\"\n",
        "      Call async call to transform documents.\n",
        "      \"\"\"\n",
        "      return asyncio.run(self._acall(documents, show_progress=False))\n",
        "\n",
        "   async def _aextract(self, doc: Document) -> Document:\n",
        "      \"\"\"\n",
        "      Extract triples from a document.\n",
        "      \"\"\"\n",
        "      assert hasattr(doc, \"page_content\")\n",
        "      text = doc.page_content\n",
        "      try:\n",
        "         llm_response = await self.llm.ainoke(\n",
        "            self.extract_prompt.format(\n",
        "               text=text, \n",
        "               max_knowledge_triplets=self.max_knowledge_triples\n",
        "            )\n",
        "         )\n",
        "         entities, entities_relationship = self.parse_fn(llm_response)\n",
        "      except ValueError:\n",
        "         entities = []\n",
        "         entities_relationship = []\n",
        "      # Pop the 'nodes' and 'relations' key from the document metadata dictionary\n",
        "      existing_nodes = doc.metadata.pop(KG_NODES_KEY, [])\n",
        "      existing_relations = doc.metadata.pop(KG_RELATIONS_KEY, [])\n",
        "      metadata = doc.metadata.copy()\n",
        "      for entity, entity_type, description in entities:\n",
        "         metadata[\n",
        "            \"entity_description\"\n",
        "         ] = description\n",
        "         entity_node = EntityNode\n",
        "   \n",
        "   async def _acall(\n",
        "      self, \n",
        "      documents: list[Document],\n",
        "      show_progress: bool = False\n",
        "   ) -> list[Document]:\n",
        "      jobs = []\n",
        "      for doc in documents:\n",
        "         jobs.append[self._aextract(doc)]\n",
        "      return await run_jobs(\n",
        "         jobs,\n",
        "         workers=self.num_workers,\n",
        "         show_progress=show_progress,\n",
        "         desc = \"Extracting paths from text\"\n",
        "      )\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 50,
      "metadata": {
        "gather": {
          "logged": 1742076290141
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "await llm.ainvoke(\"Tell me a joke\")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 47,
          "data": {
            "text/plain": "AIMessage(content='Why did the scarecrow win an award?\\n\\nBecause he was outstanding in his field!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 11, 'total_tokens': 28, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b705f0c291', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-7e018276-5f7f-4abe-8c3f-885e34adee66-0', usage_metadata={'input_tokens': 11, 'output_tokens': 17, 'total_tokens': 28, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 47,
      "metadata": {
        "gather": {
          "logged": 1742075714591
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "await llm.ainvoke(DEFAULT_KG_TRIPLET_EXTRACT_PROMPT.format(text=news_documents[1], max_knowledge_triplets=10))"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 49,
          "data": {
            "text/plain": "AIMessage(content='(FirstEnergy, posted, earnings results)  \\n(FirstEnergy, reported, $0.53 earnings per share)  \\n(FirstEnergy, topped, consensus estimate)  \\n(FirstEnergy, had, net margin of 10.85%)  \\n(FirstEnergy, had, return on equity of 17.17%)  \\n(earnings results, reported on, Tuesday)  \\n(consensus estimate, was, $0.52)  \\n($0.01, was, difference)  \\n(utilities provider, is, FirstEnergy)  \\n(RTT News, reports, earnings results)', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 118, 'prompt_tokens': 243, 'total_tokens': 361, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b705f0c291', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-bd028b0e-79ec-411f-b85d-bf04a040d188-0', usage_metadata={'input_tokens': 243, 'output_tokens': 118, 'total_tokens': 361, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 49,
      "metadata": {
        "gather": {
          "logged": 1742075814845
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 29,
          "data": {
            "text/plain": "Document(metadata={}, page_content='FirstEnergy (NYSE:FE – Get Rating) posted its earnings results on Tuesday. The utilities provider reported $0.53 earnings per share for the quarter, topping the consensus estimate of $0.52 by $0.01, RTT News reports. FirstEnergy had a net margin of 10.85% and a return on equity of 17.17%. During the same period...\\nIf the content contained herein violates any of your rights, including those of copyright, you are requested to immediately notify us using via the following email address operanews-external(at)opera.com\\nTop News')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1742071011313
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = DEFAULT_KG_TRIPLET_EXTRACT_PROMPT.format(max_knowledge_triplets=10, text=news_documents[0].page_content)\n",
        "prompt"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 31,
          "data": {
            "text/plain": "\"Some text is provided below. Given the text, extract up to 10 knowledge triplets in the form of (subject, predicate, object). Avoid stopwords.\\n---------------------\\nExample:Text: Alice is Bob's mother.Triplets:\\n(Alice, is mother of, Bob)\\nText: Philz is a coffee shop founded in Berkeley in 1982.\\nTriplets:\\n(Philz, is, coffee shop)\\n(Philz, founded in, Berkeley)\\n(Philz, founded in, 1982)\\n---------------------\\nText: JHVEPhoto Like many companies in the O&G sector, the stock of Chevron (NYSE:CVX) has declined about 10% over the past 90-days despite the fact that Q2 consensus earnings estimates have risen sharply (~25%) during that same time frame. Over the years, Chevron has kept a very strong balance sheet. That allowed the...\\nTriplets:\\n\""
          },
          "metadata": {}
        }
      ],
      "execution_count": 31,
      "metadata": {
        "gather": {
          "logged": 1742071036002
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(prompt)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 32,
          "data": {
            "text/plain": "AIMessage(content='(Chevron, has declined, about 10%)\\n(Chevron, is, stock)\\n(Chevron, has, strong balance sheet)\\n(Q2 consensus earnings estimates, have risen, sharply)\\n(earnings estimates, risen during, same time frame)\\n(Chevron, is in, O&G sector)\\n(Chevron, has, declined over past 90-days)\\n(10%, is, decline)\\n(Q2, is, estimate)\\n(25%, is, rise)', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 97, 'prompt_tokens': 192, 'total_tokens': 289, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b705f0c291', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-d4ae0e2e-2105-412a-8c07-6aa93f1a4367-0', usage_metadata={'input_tokens': 192, 'output_tokens': 97, 'total_tokens': 289, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 32,
      "metadata": {
        "gather": {
          "logged": 1742071060801
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "print(langchain.__version__)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0.3.20\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1742048365154
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identity Communities"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarize Communities"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Global Answer\n",
        "\n",
        "The following is a simple example to show that the LLM generation is grounded to the retrieved triplets. Compared to the answer directly generated by the LLM, the RAG answer is more grounded to the knowledge stored in the Vector Store about Einstein. Besides, the generation does not seem to identify the knowledge clique, which is supposed to be a strength of Graph Rag. For example, the facts about Theory of Relativity is a theory in physics and it is developed in early 20th century is not reflected in the answer.\n",
        "\n",
        "\n",
        "```\n",
        "{\n",
        "    'query': 'Tell me about Albert Einstein', \n",
        "    'result': \"Albert Einstein was a theoretical physicist best known for developing the Theory of Relativity, which revolutionized our understanding of space, time, and gravity. He was born on March 14, 1879, in Ulm, Germany, and later became a Swiss citizen. Einstein's work laid the foundation for modern physics, particularly his famous equation E=mc², which describes the equivalence of mass and energy. Throughout his career, he received numerous awards and honors, including the Nobel Prize in Physics in 1921 for his explanation of the photoelectric effect. Einstein passed away on April 18, 1955, but his contributions to science continue to influence the field today.\"\n",
        "}\n",
        "```\n",
        "\n",
        "The consistency is another issue. The generation takes into account the second level relations about Einstein.\n",
        "\n",
        "```\n",
        "{\n",
        "    'query': 'Tell me about Albert Einstein', \n",
        "    'result': \"Albert Einstein was a renowned physicist who was born in 1879 and passed away in 1955. He is best known for developing the Theory of Relativity, which he worked on in the early 20th century. Einstein's contributions to science have had a profound impact on our understanding of physics and the universe. His work has influenced various fields and continues to be a subject of study and admiration today.\"}\n",
        "```\n",
        "\n",
        "Answers generated by LLM to the query 'Tell me about Altert Einstein'. This is done by calling `llm.invoke('Tell me about Albert Einstein')`.\n",
        "\n",
        "```\n",
        "AIMessage(content='Albert Einstein (1879-1955) was a theoretical physicist renowned for developing the theory of relativity, one of the two pillars of modern physics alongside quantum mechanics. His work revolutionized our understanding of space, time, and energy.\\n\\n### Early Life\\nBorn on March 14, 1879, in Ulm, Germany, Einstein showed an early interest in science and mathematics. He faced challenges in his schooling due to a nonconformist attitude and struggled with rigid educational systems. He later studied at the Polytechnic Institute in Zurich, where he graduated in 1900.\\n\\n### Career Highlights\\nEinstein initially worked as a patent examiner in Bern, Switzerland, where he developed many of his groundbreaking ideas during his free time. In 1905, often referred to as his \"miracle year,\" he published four pivotal papers:\\n1. **Special Theory of Relativity** – Introduced the famous equation E=mc², establishing the relationship between mass and energy.\\n2. **Photoelectric Effect** – Provided evidence for the quantization of light, which later contributed to the development of quantum theory; this work earned him the Nobel Prize in Physics in 1921.\\n3. **Brownian Motion** – Offered explanations for the random motion of particles suspended in fluids, providing empirical support for atomic theory.\\n4. **Mass-Energy Equivalence** – Established the foundational principles that would shape nuclear physics.\\n\\nIn 1915, Einstein completed his General Theory of Relativity, which expanded on his earlier work to include gravity as a curvature of spacetime rather than a force acting at a distance. This theory predicted phenomena like the bending of light around massive objects and was confirmed by observations during a solar eclipse in 1919.\\n\\n### Later Life and Legacy\\nEinstein immigrated to the United States in 1933, fleeing the rise of Nazism in Germany. He accepted a position at the Institute for Advanced Study in Princeton, New Jersey, where he continued his work until his death on April 18, 1955. Throughout his life, Einstein was involved in various social and political causes, advocating for pacifism, civil rights, and nuclear disarmament.\\n\\nEinstein\\'s contributions laid the groundwork for much of modern physics, and his work continues to influence numerous fields, including cosmology, quantum mechanics, and theoretical physics. His iconic status and the phrase \"Einstein\" have become synonymous with genius, and his legacy endures through both his scientific achievements and his humanitarian efforts.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 503, 'prompt_tokens': 12, 'total_tokens': 515, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b705f0c291', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-bf0b8478-646f-4f87-8ce9-cc848312724d-0', usage_metadata={'input_tokens': 12, 'output_tokens': 503, 'total_tokens': 515, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
        "```\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.graphs.networkx_graph import NetworkxEntityGraph, KnowledgeTriple\n",
        "from langchain.chains import RetrievalQA # According to ChatGPT, the GraphCyberQAChain requires a Neo4jGraph backend.\n",
        "\n",
        "# Initialize the graph\n",
        "graph = NetworkxEntityGraph()\n",
        "\n",
        "# Sample knowledge to extract relationships\n",
        "triplets = [\n",
        "    KnowledgeTriple(\"Albert Einstein\", \"discovered\", \"Theory of Relativity\"),\n",
        "    KnowledgeTriple(\"Albert Einstein\", \"born in\", \"1879\"),\n",
        "    KnowledgeTriple(\"Albert Einstein\", \"die at\", \"1955\"),\n",
        "    KnowledgeTriple(\"Isaac Newton\", \"formulated\", \"Laws of Motion\"),\n",
        "    KnowledgeTriple(\"Marie Curie\", \"pioneered\", \"Radioactivity\"),\n",
        "    KnowledgeTriple(\"Theory of Relativity\", \"is\", \"Theory in Physics\"),\n",
        "    KnowledgeTriple(\"Theory of Relativity\", \"develope time\", \"early 20th century\")\n",
        "]\n",
        "\n",
        "# Add entities and relationships to the graph\n",
        "for triplet in triplets:\n",
        "    graph.add_triple(triplet)\n",
        "\n",
        "# Convert graph to document\n",
        "documents = [Document(page_content=f\"{subj} {pred} {obj}\") for subj, pred, obj in triplets]\n",
        "\n",
        "# Add documents to vector store\n",
        "vector_store.add_documents(documents)\n",
        "\n",
        "# Create a retrieval-based QA chain\n",
        "retrieval_chain = RetrievalQA.from_chain_type(llm, retriever=vector_store.as_retriever())\n",
        "\n",
        "# Ask a question\n",
        "query = \"Tell me about Albert Einstein\"\n",
        "response = retrieval_chain.invoke(query)\n",
        "\n",
        "print(response)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'query': 'Tell me about Albert Einstein', 'result': \"Albert Einstein was a renowned physicist who was born in 1879 and passed away in 1955. He is best known for developing the Theory of Relativity, which he worked on in the early 20th century. Einstein's contributions to science have had a profound impact on our understanding of physics and the universe. His work has influenced various fields and continues to be a subject of study and admiration today.\"}\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1741959963574
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"Tell me about Albert Einstein\")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "AIMessage(content='Albert Einstein was a theoretical physicist born on March 14, 1879, in Ulm, Germany, and he passed away on April 18, 1955, in Princeton, New Jersey, USA. He is best known for developing the theory of relativity, particularly the mass-energy equivalence formula \\\\(E=mc^2\\\\), which has become one of the most famous equations in physics.\\n\\nEinstein\\'s early education was in Germany, where he struggled in some subjects but excelled in mathematics and physics. He earned a diploma from the Polytechnic Institute in Zurich, Switzerland, in 1900. After a brief period of working at the Swiss Patent Office, he published several groundbreaking papers in 1905, a year often referred to as his \"annus mirabilis\" or miracle year. These papers included his work on the photoelectric effect (for which he later received the Nobel Prize in Physics in 1921), Brownian motion, and special relativity.\\n\\nIn 1915, Einstein completed his general theory of relativity, which expanded the ideas of relativity to include acceleration and gravitation. This theory revolutionized our understanding of space, time, and gravity, replacing Newtonian mechanics in many areas of physics.\\n\\nEinstein\\'s contributions extended beyond theoretical physics. He was an outspoken advocate for civil rights, pacifism, and Zionism, and he was a prominent figure in various political and social movements throughout his life. He was also known for his philosophical reflections on science and its implications.\\n\\nIn 1933, with the rise of the Nazi regime, Einstein emigrated to the United States, where he accepted a position at the Institute for Advanced Study in Princeton. He continued to work on theoretical physics until his death, but he became increasingly concerned with the implications of nuclear weapons after the atomic bomb was developed during World War II.\\n\\nEinstein\\'s legacy extends far beyond his scientific contributions. He is regarded as a symbol of genius, creativity, and humanism, and his work continues to influence modern physics and science as a whole.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 414, 'prompt_tokens': 12, 'total_tokens': 426, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_ded0d14823', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-8c237d77-abce-4018-8732-753e13193ccb-0', usage_metadata={'input_tokens': 12, 'output_tokens': 414, 'total_tokens': 426, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1741959967849
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph.get_entity_knowledge(entity=\"Albert Einstein\")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "['Albert Einstein discovered Theory of Relativity',\n 'Albert Einstein born in 1879',\n 'Albert Einstein die at 1955']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1741959968002
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enhance the retrieval using Leiden."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "from graspologic.partition import hierarchical_leiden"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'graspologic'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraspologic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hierarchical_leiden\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graspologic'"
          ]
        }
      ],
      "execution_count": 55,
      "metadata": {
        "gather": {
          "logged": 1741959856514
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Require to install graphviz\n",
        "# import matplotlib.pyplot as plt\n",
        "# import networkx as nx\n",
        "\n",
        "# # Convert to NetworkX graph\n",
        "# graph.draw_graphviz()"
      ],
      "outputs": [],
      "execution_count": 35,
      "metadata": {
        "gather": {
          "logged": 1741957991438
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install graspologic"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting graspologic\n  Downloading graspologic-3.4.1-py3-none-any.whl.metadata (5.8 kB)\nCollecting POT<0.10,>=0.9 (from graspologic)\n  Downloading POT-0.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\nCollecting anytree<3.0.0,>=2.12.1 (from graspologic)\n  Downloading anytree-2.12.1-py3-none-any.whl.metadata (8.1 kB)\nCollecting beartype<0.19.0,>=0.18.5 (from graspologic)\n  Downloading beartype-0.18.5-py3-none-any.whl.metadata (30 kB)\nCollecting gensim<5.0.0,>=4.3.2 (from graspologic)\n  Downloading gensim-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\nCollecting graspologic-native<2.0.0,>=1.2.1 (from graspologic)\n  Downloading graspologic_native-1.2.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.6 kB)\nCollecting hyppo<0.5.0,>=0.4.0 (from graspologic)\n  Downloading hyppo-0.4.0-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: joblib<2.0.0,>=1.4.2 in /anaconda/envs/rag/lib/python3.10/site-packages (from graspologic) (1.4.2)\nRequirement already satisfied: matplotlib<4.0.0,>=3.8.4 in /anaconda/envs/rag/lib/python3.10/site-packages (from graspologic) (3.9.2)\nRequirement already satisfied: networkx<4,>=3 in /anaconda/envs/rag/lib/python3.10/site-packages (from graspologic) (3.4.2)\nCollecting numpy<2.0.0,>=1.26.4 (from graspologic)\n  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\nRequirement already satisfied: scikit-learn<2.0.0,>=1.4.2 in /anaconda/envs/rag/lib/python3.10/site-packages (from graspologic) (1.5.2)\nCollecting scipy==1.12.0 (from graspologic)\n  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\nCollecting seaborn<0.14.0,>=0.13.2 (from graspologic)\n  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\nCollecting statsmodels<0.15.0,>=0.14.2 (from graspologic)\n  Downloading statsmodels-0.14.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\nRequirement already satisfied: typing-extensions<5.0.0,>=4.4.0 in /anaconda/envs/rag/lib/python3.10/site-packages (from graspologic) (4.12.2)\nCollecting umap-learn<0.6.0,>=0.5.6 (from graspologic)\n  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: six in /anaconda/envs/rag/lib/python3.10/site-packages (from anytree<3.0.0,>=2.12.1->graspologic) (1.16.0)\nCollecting smart-open>=1.8.1 (from gensim<5.0.0,>=4.3.2->graspologic)\n  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\nCollecting numba>=0.46 (from hyppo<0.5.0,>=0.4.0->graspologic)\n  Downloading numba-0.61.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\nCollecting autograd>=1.3 (from hyppo<0.5.0,>=0.4.0->graspologic)\n  Downloading autograd-1.7.0-py3-none-any.whl.metadata (7.5 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /anaconda/envs/rag/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.8.4->graspologic) (1.3.0)\nRequirement already satisfied: cycler>=0.10 in /anaconda/envs/rag/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.8.4->graspologic) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /anaconda/envs/rag/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.8.4->graspologic) (4.53.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in /anaconda/envs/rag/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.8.4->graspologic) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /anaconda/envs/rag/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.8.4->graspologic) (24.1)\nRequirement already satisfied: pillow>=8 in /anaconda/envs/rag/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.8.4->graspologic) (10.4.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /anaconda/envs/rag/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.8.4->graspologic) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.7 in /anaconda/envs/rag/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.8.4->graspologic) (2.9.0)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /anaconda/envs/rag/lib/python3.10/site-packages (from scikit-learn<2.0.0,>=1.4.2->graspologic) (3.5.0)\nRequirement already satisfied: pandas>=1.2 in /anaconda/envs/rag/lib/python3.10/site-packages (from seaborn<0.14.0,>=0.13.2->graspologic) (2.2.3)\nCollecting patsy>=0.5.6 (from statsmodels<0.15.0,>=0.14.2->graspologic)\n  Downloading patsy-1.0.1-py2.py3-none-any.whl.metadata (3.3 kB)\nCollecting pynndescent>=0.5 (from umap-learn<0.6.0,>=0.5.6->graspologic)\n  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: tqdm in /anaconda/envs/rag/lib/python3.10/site-packages (from umap-learn<0.6.0,>=0.5.6->graspologic) (4.66.5)\nCollecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.46->hyppo<0.5.0,>=0.4.0->graspologic)\n  Downloading llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\nRequirement already satisfied: pytz>=2020.1 in /anaconda/envs/rag/lib/python3.10/site-packages (from pandas>=1.2->seaborn<0.14.0,>=0.13.2->graspologic) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /anaconda/envs/rag/lib/python3.10/site-packages (from pandas>=1.2->seaborn<0.14.0,>=0.13.2->graspologic) (2024.1)\nRequirement already satisfied: wrapt in /anaconda/envs/rag/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.2->graspologic) (1.16.0)\nDownloading graspologic-3.4.1-py3-none-any.whl (5.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading anytree-2.12.1-py3-none-any.whl (44 kB)\nDownloading beartype-0.18.5-py3-none-any.whl (917 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m917.8/917.8 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gensim-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading graspologic_native-1.2.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (365 kB)\nDownloading hyppo-0.4.0-py3-none-any.whl (146 kB)\nDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading POT-0.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (865 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.6/865.6 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\nDownloading statsmodels-0.14.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\nDownloading autograd-1.7.0-py3-none-any.whl (52 kB)\nDownloading numba-0.61.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading patsy-1.0.1-py2.py3-none-any.whl (232 kB)\nDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\nDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\nDownloading llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: smart-open, numpy, llvmlite, graspologic-native, beartype, anytree, scipy, patsy, numba, autograd, statsmodels, POT, gensim, seaborn, pynndescent, hyppo, umap-learn, graspologic\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.0.2\n    Uninstalling numpy-2.0.2:\n      Successfully uninstalled numpy-2.0.2\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.14.1\n    Uninstalling scipy-1.14.1:\n      Successfully uninstalled scipy-1.14.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.18.0 requires keras>=3.5.0, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed POT-0.9.5 anytree-2.12.1 autograd-1.7.0 beartype-0.18.5 gensim-4.3.3 graspologic-3.4.1 graspologic-native-1.2.3 hyppo-0.4.0 llvmlite-0.44.0 numba-0.61.0 numpy-1.26.4 patsy-1.0.1 pynndescent-0.5.13 scipy-1.12.0 seaborn-0.13.2 smart-open-7.1.0 statsmodels-0.14.4 umap-learn-0.5.7\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 56,
      "metadata": {
        "gather": {
          "logged": 1741959905149
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras>=3.5.0"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Note: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 57,
      "metadata": {
        "gather": {
          "logged": 1741959942735
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "rag"
    },
    "kernelspec": {
      "name": "rag",
      "language": "python",
      "display_name": "rag"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}